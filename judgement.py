# -*- coding: utf-8 -*-
"""judgement.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QE3CGmxyX-GBcLyZlT28tmGOZw2XHS_r
"""


from bs4 import BeautifulSoup as bs
import json
import numpy as np
import pandas as pd
import requests

def lawsuitUrlGrabber_guilty(url):
  r = requests.get(url.format(1))
  page = bs(r.text, "html.parser")
  pageNum = int(page.find(id = 'hlLast', href = True)['href'].split("page=",1)[1])
  for i in range(1, pageNum+1):
    r = requests.get(url.format(i))
    law = bs(r.text, "html.parser")
    for i in law.find_all('a', href = True):
      if 'data' in i['href']:
        link = "https://law.judicial.gov.tw/FJUD/"+i['href']
        if link in urlLIST_1:
          continue
        else:
          urlLIST_1.append(link)
def lawsuitUrlGrabber_notguilty(url):
  if url[-1] != 'n':
    r = requests.get(url)
    law = bs(r.text, "html.parser")
    for i in law.find_all('a', href = True):
      if 'data' in i['href']:
        link = "https://law.judicial.gov.tw/FJUD/"+i['href']
        if link in urlLIST_2:
          continue
        else:
          urlLIST_2.append(link)
  else:
    r = requests.get(url.format(1))
    page = bs(r.text, "html.parser")
    pageNum = int(page.find(id = 'hlLast', href = True)['href'].split("page=",1)[1])
    for i in range(1, pageNum+1):
      r = requests.get(url.format(i))
      law = bs(r.text, "html.parser")
      for i in law.find_all('a', href = True):
        if 'data' in i['href']:
          link = "https://law.judicial.gov.tw/FJUD/"+i['href']
          if link in urlLIST_2:
            continue
          else:
            urlLIST_2.append(link)
def data_cleaner(string):
    textLIST = [x.replace("\r","").replace("\n","").replace("\t","").replace("\u3000","").replace("\xa0", "").replace(" ", "") for x in string]
    return "".join(textLIST)
"""**Note:**
1. 近十年判決書會存在htmlcontent標籤
2. 十年以前的判決書會存在text-pre text-pre-in標籤
"""
def judgementGrabber_1(judgeUrl):
  judgementDICT = {"ID": "",
                   "Date": "", 
                   "Charge": "",
                   "plaintiff": "",
                   "guilty": 1, 
                   "sentence_1": "", 
                   "judgement_1": "", 
                   "sentence_2": "", 
                   "judgement_2": "", 
                   "url": judgeUrl}
  r = requests.get(judgeUrl)
  lawsuit = bs(r.text, "html.parser")
  judgementDICT['ID'] = data_cleaner(lawsuit.find("title").text)
  judgementDICT['Date'] = data_cleaner(lawsuit.find_all(class_ = 'col-td')[1].text) 
  judgementDICT['Charge'] = data_cleaner(lawsuit.find_all(class_ = 'col-td')[2].text)
  judgement = lawsuit.find(class_ = "text-pre text-pre-in").text # 尋找在text-pre text-pre-in標籤裡的判決書
  judgementDICT['judgement_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實") [2]
  if '犯罪' == data_cleaner(judgement).partition("主文")[2].partition("事實") [0][-2:]:
    judgementDICT['sentence_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0][:-3]
  else:
    judgementDICT['sentence_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0]  
  judgement = lawsuit.find(class_ = "htmlcontent").text # 尋找在htmlcontent標籤裡的判決書
  judgementDICT['judgement_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實") [2]
  if '犯罪' == data_cleaner(judgement).partition("主文")[2].partition("事實") [0][-2:]: #將犯罪二字去除
    judgementDICT['sentence_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0][:-3]
  else:
    judgementDICT['sentence_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0]  
  return judgementDICT
def judgementGrabber_2(judgeUrl):
  judgementDICT = {"ID": "", 
                   "Date": "", 
                   "Charge": "",
                   "plaintiff": "",
                   "defendent": "",
                   "guilty": 0, 
                   "sentence_1": "", 
                   "judgement_1": "", 
                   "sentence_2": "", 
                   "judgement_2": "", 
                   "url": judgeUrl}
  r = requests.get(judgeUrl)
  lawsuit = bs(r.text, "html.parser")
  judgementDICT['ID'] = data_cleaner(lawsuit.find("title").text)
  judgementDICT['Date'] = data_cleaner(lawsuit.find_all(class_ = 'col-td')[1].text) 
  judgementDICT['Charge'] = data_cleaner(lawsuit.find_all(class_ = 'col-td')[2].text)
  judgement = lawsuit.find(class_ = "text-pre text-pre-in").text
  judgementDICT['judgement_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實") [2]
  if '犯罪' == data_cleaner(judgement).partition("主文")[2].partition("事實") [0][-2:]:
    judgementDICT['sentence_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0][:-3]
  else:
    judgementDICT['sentence_1'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0]  
  judgement = lawsuit.find(class_ = "htmlcontent").text
  judgementDICT['judgement_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實") [2]
  if '犯罪' == data_cleaner(judgement).partition("主文")[2].partition("事實") [0][-2:]:
    judgementDICT['sentence_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0][:-3]
  else:
    judgementDICT['sentence_2'] = data_cleaner(judgement).partition("主文")[2].partition("事實")[0] 
  return judgementDICT
"""
for i in judgementLIST:
  print(i)

for i in judgementLIST:
  if '男女朋友' in i['judgement_1']:
    txt = i['judgement_1'].split("，")
    # foo = somevalue
    previous = next_ = None
    l = len(txt)
    for index, obj in enumerate(txt):
      if '男女朋友' in obj:
        print("判決書裡有男女朋友的判決編號:", judgementLIST.index(i))
        print("有男女朋友的句子，第{}句".format(index))
        print("Guilty:", i['guilty'])
        print("url:", i['url'])
        print("判決結果:", i['sentence_1'])
        if index > 0:
            pp = txt[index - 2]
            previous = txt[index - 1]
            print("有男女朋友的句子的上上一句:", pp)
            print("有男女朋友的句子的上一句:", previous)
        print("有男女朋友的句子:", txt[index])
        if index < (l - 1):
            nn = txt[index + 2]
            next_ = txt[index + 1]
            print("有男女朋友的句子的下一句:", next_)
            print("有男女朋友的句子的下下一句", nn)
        print("----------------------")
"""
def data_collector(inputDICT):
  lawsuitDICT = {'ID': "",'Date': '','Guilty': 0, '心理諮商': 0, '鑑定書': 0, '測謊鑑定': 0, '驗傷診斷書': 0, '通話紀錄': 0, '對話紀錄': 0, '坦承': 0, '指述': 0, '繪圖': 0, '輔導紀錄': 0, '同事關係': 0, '同居關係': 0, '按摩': 0, '男女朋友': 0}
  lawsuitDICT['ID'] = inputDICT['ID']
  lawsuitDICT['Guilty'] = inputDICT['guilty']
  lawsuitDICT['Date'] = inputDICT['Date']
  for i in range(1, 3):
    if '心理諮商' in inputDICT['judgement_{}'.format(i)]:
      lawsuitDICT['心理諮商'] = 1
    if '鑑定書' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['鑑定書'] = 1
    if '測謊鑑定' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['測謊鑑定'] = 1
    if '驗傷診斷書' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['驗傷診斷書'] = 1
    if '通話紀錄' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['通話紀錄'] = 1
    if '對話紀錄' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['對話紀錄'] = 1
    if '坦承' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['坦承'] = 1
    if '指述' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['指述'] = 1
    if '繪圖' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['繪圖'] = 1
    if '輔導紀錄' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['輔導紀錄'] = 1
    if '同事關係' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['同事關係'] = 1
    if '按摩' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['按摩'] = 1
    if '男女朋友' in lawsuit['judgement_{}'.format(i)]:
      lawsuitDICT['男女朋友'] = 1
  return lawsuitDICT


"""**Notes & Problems**
1. 心理諮商：不一定是告訴人接受心理諮商，也有可能是證人接受心理諮商。
2. 鑑定書專指DNA鑑定處理
3. 告訴人與被告人皆會接受測謊鑑定
4. 驗傷診斷書：純粹判斷證據有無，無疑慮
5. 坦承：被告認罪。指述：告訴人指認描述（需要算詞頻嗎？）
6. 繪圖：純粹判斷證據有無，無疑慮
7. 輔導紀錄：是否有可能被告也有輔導紀錄？Ａ：排除論罪科刑後（不是影響有沒有罪，只會影響刑期）
8. 仍須判斷同事關係、同居關係、按摩，與男女朋友是否為被告與告訴人之間

"""
if __name__ == '__main__':
  url_1 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=bbde96d11f26de49e018a79c898151c6&sort=DS&page={}&ot=in" #1 guilty 100/01/01~105/12/31
  url_2 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=7b807db4e6d042f04f917a6f070028fb&sort=DS&page={}&ot=in" #1 guilty 106/01/01~110/5/13
  url_3 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=980ef39c3a6772786a42df5c76e79a36&sort=DS&page={}&ot=in" #2 guilty 100/01/01~103/12/31
  url_4 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=f403fe9cce4affed23dfba6ed1629ba2&sort=DS&page={}&ot=in" #2 guilty 104/01/01~110/05/13
  url_5 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?ty=JUDBOOK&q=799cc1a3bbcdaad69acb2af5104d8518" #3 not guilty 100/01/01~110/05/13
  url_6 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=59479c327bc46bebb3dc194975d41f31&sort=DS&page={}&ot=in" #4 not guilty 100/01/01~110/05/13 
  url_7 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=73cfccdf45909942061637070af448d5&sort=DS&page={}&ot=in" #5 guilty 100/01/01~110/05/13 
  url_8 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=325c0f4ba743be3a4da73481c57eb7b6&sort=DS&page={}&ot=in" #6 guilty 100/01/01~110/05/13
  url_9 = "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?ty=JUDBOOK&q=6e595212546038044de5beb70c1e90ed" #7 not guilty 100/01/01~110/05/13
  guiltyLIST = ["https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=bbde96d11f26de49e018a79c898151c6&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=7b807db4e6d042f04f917a6f070028fb&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=980ef39c3a6772786a42df5c76e79a36&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=f403fe9cce4affed23dfba6ed1629ba2&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=73cfccdf45909942061637070af448d5&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=325c0f4ba743be3a4da73481c57eb7b6&sort=DS&page={}&ot=in"]
  notguiltyLIST = ["https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?ty=JUDBOOK&q=799cc1a3bbcdaad69acb2af5104d8518", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?q=59479c327bc46bebb3dc194975d41f31&sort=DS&page={}&ot=in", "https://law.judicial.gov.tw/FJUD/qryresultlst.aspx?ty=JUDBOOK&q=6e595212546038044de5beb70c1e90ed"]
  urlLIST_1 = [] #guilty
  urlLIST_2 = [] #not guilty
  for i in guiltyLIST:
    lawsuitUrlGrabber_guilty(i)
  for j in notguiltyLIST:
    lawsuitUrlGrabber_notguilty(j)
  print(len(urlLIST_1), len(urlLIST_2))
  judgementLIST = []
  urlLIST = urlLIST_1+urlLIST_2
  urlLen = len(urlLIST)
  for i in urlLIST_1:
    urlIndex = urlLIST.index(i)
    if urlIndex == int(urlLen*0.15):
      print("已爬取25%的網頁")
    if urlIndex == int(urlLen*0.30):
      print("已爬取50%的網頁")
    if urlIndex == int(urlLen*0.45):
      print("已爬取75%的網頁")
    judgementDICT = judgementGrabber_1(i)
    judgementLIST.append(judgementDICT)
  for i in urlLIST_2:
    urlIndex = len(urlLIST_1) + urlLIST_2.index(i)
    if urlIndex == int(urlLen*0.60):
      print("已爬取60%的網頁")
    if urlIndex == int(urlLen*0.75):
      print("已爬取75%的網頁")
    if urlIndex == int(urlLen*0.90):
      print("已爬取90%的網頁")
    judgementDICT = judgementGrabber_2(i)
    judgementLIST.append(judgementDICT)
  print("爬取完畢")
  with open("judgement.json", "w", encoding = "utf-8") as f:
    json.dump(judgementLIST, f, ensure_ascii = False, indent = 4)
  lawsuitLIST = []
  for lawsuit in judgementLIST:
    lawsuitLIST.append(data_collector(lawsuit))
  df = pd.DataFrame(lawsuitLIST)
  df.to_csv("result.csv")